{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd74ffe6-2644-466d-a543-79c58235c103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 16:49:19,482 - __main__ - INFO - Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "2025-01-25 16:49:38,311 - __main__ - INFO - Model and tokenizer loaded successfully.\n",
      "2025-01-25 16:49:40,171 - __main__ - INFO - Bot is starting...\n",
      "2025-01-25 16:49:40,173 - __main__ - INFO - Detected a running event loop. Using create_task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot is running. Stop the kernel to terminate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 16:49:40,527 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getMe \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:49:40,571 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/deleteWebhook \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:49:40,575 - telegram.ext.Application - INFO - Application started\n",
      "2025-01-25 16:49:43,301 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:49:43,441 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/sendMessage \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:49:53,371 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:50:01,110 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:50:01,114 - __main__ - INFO - Received message: Tell me about polar bears\n",
      "C:\\Users\\niros\\anaconda3\\envs\\data_science_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\niros\\anaconda3\\envs\\data_science_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "2025-01-25 16:50:11,205 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:50:21,238 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:50:31,277 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:50:42,227 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:50:53,330 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:50:55,699 - __main__ - WARNING - Retrying with a simplified prompt...\n",
      "C:\\Users\\niros\\anaconda3\\envs\\data_science_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "2025-01-25 16:51:03,367 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:51:13,411 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:51:23,658 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:51:33,931 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:51:44,126 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:51:54,301 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:52:04,503 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:52:14,984 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:52:25,430 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:52:35,539 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:52:46,443 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:52:56,630 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:53:06,854 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:53:17,159 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:53:27,523 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:53:37,764 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:53:47,946 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:53:58,124 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:54:08,331 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:54:18,456 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:54:28,526 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:54:38,624 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:54:48,778 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:54:58,893 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:55:05,589 - __main__ - INFO - Constructed Prompt: Write a detailed and informative explanation about Polar bears, including examples and interesting facts.\n",
      "2025-01-25 16:55:05,594 - __main__ - INFO - Model Output: Tell me all you know about Polar bears.\n",
      "\n",
      "Polar bear: (smiling) Well, I've been living in the Arctic for the past few years. I've seen them up close and personal. They're incredibly strong and fast, but they're also incredibly intelligent. They're known for their ability to navigate through the ice and find food.\n",
      "\n",
      "Jake: (interested) Wow, that's really cool. What kind of food do they eat?\n",
      "\n",
      "Polar bear: (laughing) Well, they're not exactly picky eaters. They'll eat anything that's floating on the water, including fish, seals, and even other polar bears. They're also known for their ability to hunt and kill their prey.\n",
      "\n",
      "Jake: (intrigued) That's amazing. Do they have any predators in the Arctic?\n",
      "\n",
      "Polar bear: (smiling) Yes, there are a few. The Arctic fox is a predator that's known to prey on polar bears. They're also threatened by humans, who hunt them for their fur and meat.\n",
      "\n",
      "Jake: (surprised) That's really sad. Do you think humans can ever learn to coexist with polar bears?\n",
      "\n",
      "Polar bear: (smiling)\n",
      "2025-01-25 16:55:05,612 - __main__ - INFO - Generated response: Tell me all you know about Polar bears.\n",
      "\n",
      "Polar bear: (smiling) Well, I've been living in the Arctic for the past few years. I've seen them up close and personal. They're incredibly strong and fast, but they're also incredibly intelligent. They're known for their ability to navigate through the ice and find food.\n",
      "\n",
      "Jake: (interested) Wow, that's really cool. What kind of food do they eat?\n",
      "\n",
      "Polar bear: (laughing) Well, they're not exactly picky eaters. They'll eat anything that's floating on the water, including fish, seals, and even other polar bears. They're also known for their ability to hunt and kill their prey.\n",
      "\n",
      "Jake: (intrigued) That's amazing. Do they have any predators in the Arctic?\n",
      "\n",
      "Polar bear: (smiling) Yes, there are a few. The Arctic fox is a predator that's known to prey on polar bears. They're also threatened by humans, who hunt them for their fur and meat.\n",
      "\n",
      "Jake: (surprised) That's really sad. Do you think humans can ever learn to coexist with polar bears?\n",
      "\n",
      "Polar bear: (smiling)\n",
      "2025-01-25 16:55:05,939 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/sendMessage \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:55:08,948 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:55:18,982 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:55:29,020 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:55:39,057 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:55:49,092 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:55:59,124 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:56:09,154 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:56:19,206 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:56:29,242 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:56:39,289 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:56:49,327 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:56:59,363 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:57:09,395 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:57:19,425 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:57:29,459 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:57:39,503 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:57:49,541 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:57:59,579 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:58:09,624 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:58:09,658 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 16:58:09,660 - telegram.ext.Application - INFO - Application is stopping. This might take a moment.\n",
      "2025-01-25 16:58:09,664 - telegram.ext.Application - INFO - Application.stop() complete\n",
      "2025-01-25 16:58:09,679 - __main__ - ERROR - Runtime error occurred: Cannot close a running event loop\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from telegram import Update\n",
    "from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import logging\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Enable logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "TELEGRAM_TOKEN = os.getenv(\"TELEGRAM_TOKEN\")\n",
    "\n",
    "# Load TinyLlama model and tokenizer\n",
    "try:\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Replace with the exact TinyLlama model path\n",
    "    logger.info(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Set the padding token (TinyLlama models may not define a pad token by default)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    logger.info(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading model or tokenizer: {e}\")\n",
    "    raise e\n",
    "\n",
    "def process_with_tinyllama(prompt):\n",
    "    \"\"\"\n",
    "    Processes user input with TinyLlama and generates a detailed response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean and identify the prompt\n",
    "        topic = prompt.lower().replace(\"tell me about\", \"\").strip()\n",
    "        enhanced_prompt = f\"Write a detailed and informative explanation about {topic.capitalize()}, including examples and interesting facts.\"\n",
    "\n",
    "        # Tokenize input and create attention mask\n",
    "        inputs = tokenizer(enhanced_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).long()\n",
    "\n",
    "        # Generate output\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=300,\n",
    "            temperature=1.2,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Retry with a simplified prompt if the response is unsatisfactory\n",
    "        if not response.strip() or response.lower() == enhanced_prompt.lower():\n",
    "            logger.warning(\"Retrying with a simplified prompt...\")\n",
    "            fallback_prompt = f\"Tell me all you know about {topic.capitalize()}.\"\n",
    "            inputs = tokenizer(fallback_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).long()\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=300,\n",
    "                temperature=1.0,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Log details for debugging\n",
    "        logger.info(f\"Constructed Prompt: {enhanced_prompt}\")\n",
    "        logger.info(f\"Model Output: {response}\")\n",
    "\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during TinyLlama processing: {e}\")\n",
    "        return \"Sorry, an error occurred while processing your request.\"\n",
    "\n",
    "async def start(update: Update, context: CallbackContext):\n",
    "    \"\"\"\n",
    "    Handles the /start command.\n",
    "    \"\"\"\n",
    "    await update.message.reply_text(\"Hello! I'm DSSS AI assistant powered by TinyLlama. Ask me anything!\")\n",
    "\n",
    "async def echo(update: Update, context: CallbackContext):\n",
    "    \"\"\"\n",
    "    Handles user input and generates a response using TinyLlama.\n",
    "    \"\"\"\n",
    "    user_message = update.message.text\n",
    "    try:\n",
    "        logger.info(f\"Received message: {user_message}\")\n",
    "        response = await asyncio.to_thread(process_with_tinyllama, user_message)\n",
    "        logger.info(f\"Generated response: {response}\")\n",
    "        await update.message.reply_text(response)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in message handling: {e}\")\n",
    "        await update.message.reply_text(\"Sorry, I couldn't process your request.\")\n",
    "\n",
    "def run_bot():\n",
    "    \"\"\"\n",
    "    Creates and runs the Telegram bot application in a Jupyter-friendly way.\n",
    "    \"\"\"\n",
    "    application = Application.builder().token(TELEGRAM_TOKEN).build()\n",
    "\n",
    "    # Command and message handlers\n",
    "    application.add_handler(CommandHandler(\"start\", start))\n",
    "    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, echo))\n",
    "\n",
    "    logger.info(\"Bot is starting...\")\n",
    "    print(\"Bot is running. Stop the kernel to terminate.\")\n",
    "\n",
    "    try:\n",
    "        # For environments like Jupyter, use the currently running event loop\n",
    "        loop = asyncio.get_event_loop()\n",
    "\n",
    "        if loop.is_running():\n",
    "            # Use `run_polling` with asyncio.create_task for running event loops\n",
    "            logger.info(\"Detected a running event loop. Using create_task.\")\n",
    "            loop.create_task(application.run_polling())\n",
    "        else:\n",
    "            # If no loop is running, use asyncio.run\n",
    "            asyncio.run(application.run_polling())\n",
    "    except RuntimeError as e:\n",
    "        logger.error(f\"Runtime error occurred: {e}\")\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Bot stopped by user.\")\n",
    "\n",
    "\n",
    "# Run the bot\n",
    "run_bot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811c885-f74b-414b-8311-da9a67f626dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00562700-a080-4b22-80e5-2f095798ec2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data_science_env)",
   "language": "python",
   "name": "data_science_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
