{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8037ef1f-9d51-4f31-8a7a-e3796b560ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 14:41:44,427 - __main__ - INFO - Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "2025-01-25 14:42:21,408 - __main__ - INFO - Model and tokenizer loaded successfully.\n",
      "2025-01-25 14:42:24,519 - __main__ - INFO - Bot is starting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot is running. Stop the kernel to terminate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 14:42:24,738 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getMe \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 14:42:24,775 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/deleteWebhook \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 14:42:24,780 - telegram.ext.Application - INFO - Application started\n",
      "2025-01-25 14:42:26,492 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 14:42:26,580 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/sendMessage \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 14:42:36,557 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 14:42:39,918 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 14:42:39,920 - __main__ - INFO - Received message: Tell me about elephants\n",
      "2025-01-25 14:42:49,985 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 14:43:00,023 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 14:43:10,069 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n",
      "2025-01-25 14:43:14,306 - __main__ - WARNING - Retrying with a simplified prompt...\n",
      "2025-01-25 14:43:23,496 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot7027664349:AAGle-6J7OonsOjCrG7aRaH4aBADDT1btBE/getUpdates \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from telegram import Update\n",
    "from tqdm import tqdm\n",
    "from telegram.ext import Application, CommandHandler, MessageHandler, filters, CallbackContext\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import logging\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Enable logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "TELEGRAM_TOKEN = os.getenv(\"TELEGRAM_TOKEN\")\n",
    "\n",
    "# Load TinyLlama model and tokenizer\n",
    "try:\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Replace with the exact TinyLlama model path\n",
    "    logger.info(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Set the padding token (TinyLlama models may not define a pad token by default)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    logger.info(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading model or tokenizer: {e}\")\n",
    "    raise e\n",
    "\n",
    "def process_with_tinyllama(prompt):\n",
    "    \"\"\"\n",
    "    Processes user input with TinyLlama and generates a detailed response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean and simplify the prompt\n",
    "        topic = prompt.lower().replace(\"tell me about\", \"\").strip()\n",
    "        enhanced_prompt = f\"Write a detailed and informative explanation about {topic.capitalize()}, including examples and interesting facts.\"\n",
    "\n",
    "        # Tokenize input and create attention mask\n",
    "        inputs = tokenizer(enhanced_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).long()\n",
    "\n",
    "        # Generate output\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=300,\n",
    "            temperature=1.2,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Retry with a simplified prompt if the response is unsatisfactory\n",
    "        if not response.strip() or response.lower() == enhanced_prompt.lower():\n",
    "            logger.warning(\"Retrying with a simplified prompt...\")\n",
    "            fallback_prompt = f\"Tell me all you know about {topic.capitalize()}.\"\n",
    "            inputs = tokenizer(fallback_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=300,\n",
    "                temperature=1.0,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Log details for debugging\n",
    "        logger.info(f\"Constructed Prompt: {enhanced_prompt}\")\n",
    "        logger.info(f\"Model Output: {response}\")\n",
    "\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during TinyLlama processing: {e}\")\n",
    "        return \"Sorry, an error occurred while processing your request.\"\n",
    "\n",
    "# Main function\n",
    "def run_bot():\n",
    "    \"\"\"\n",
    "    Creates and runs the Telegram bot application.\n",
    "    \"\"\"\n",
    "    application = Application.builder().token(TELEGRAM_TOKEN).build()\n",
    "\n",
    "    # Command and message handlers\n",
    "    application.add_handler(CommandHandler(\"start\", start))\n",
    "    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, echo))\n",
    "\n",
    "    logger.info(\"Bot is starting...\")\n",
    "    print(\"Bot is running. Stop the kernel to terminate.\")\n",
    "    asyncio.get_event_loop().run_until_complete(application.run_polling())\n",
    "\n",
    "# Run the bot\n",
    "run_bot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74ffe6-2644-466d-a543-79c58235c103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95294da5-51f8-43ca-9cb9-7c7255155023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data_science_env)",
   "language": "python",
   "name": "data_science_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
